{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package punkt to /home/moon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import collections\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.initializers import *\n",
    "from keras.activations import *\n",
    "import tensorflow as tf\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from khaiii import KhaiiiApi\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "api = KhaiiiApi()\n",
    "\n",
    "# Transformer Parameters\n",
    "d_model = 512 # Embedding Demension\n",
    "d_ff = 2048 # Feed-Forward Network's Hidden Size\n",
    "d_k = d_v = 64 # = d_model / head\n",
    "N = 6 # Num of Encoder / Decoder Layer's Stack 6\n",
    "head = 8 # Num of Multi-Head Attention's Head 8\n",
    "len_limit = 50\n",
    "\n",
    "dropout = 0.1 # Dropout\n",
    "warmup_steps = 4000 # Using When Evaluate Learning Rate\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "data_directory = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Eng2KorData:\n",
    "    def __init__(self, data_dir):\n",
    "        self.files = os.listdir(data_dir)\n",
    "        self.files = [os.path.join(data_dir, f) for f in self.files]\n",
    "        self.eng = []\n",
    "        self.kor = []\n",
    "        self.datas = []\n",
    "        \n",
    "    def data_processing(self):\n",
    "        print(\"※ Data Processing...\")\n",
    "        self.make_data_to_list()\n",
    "        self.split_eng_kor()\n",
    "        \n",
    "        return self.eng, self.kor\n",
    "        \n",
    "    def make_data_to_list(self):\n",
    "        print(\">> Read Files...\")\n",
    "        for file in self.files:\n",
    "            try:\n",
    "                with open(file, 'r', encoding='cp949') as f:\n",
    "                    all_data = f.readlines()\n",
    "                    \n",
    "                    for data in all_data:\n",
    "                        data = data.replace('\\n', '')\n",
    "                        data = data.replace('#', '')\n",
    "                                       \n",
    "                        if len(data) != 0:\n",
    "                            self.datas.append(data)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "    def split_eng_kor(self):        \n",
    "        for i in range(len(self.datas) - 3):\n",
    "            if self.datas[i][0] == '[':\n",
    "                if self.datas[i + 3][0] == '[': # Best Case\n",
    "                    self.eng.append(clean_text(self.datas[i + 1]))\n",
    "                    self.kor.append(clean_text(self.datas[i + 2]))\n",
    "                    \n",
    "\n",
    "    def make_dictionary(self, data, Korean=False):\n",
    "        print(\">> Make Dictionary...\")\n",
    "        words = []\n",
    "        for sentence in data:\n",
    "            if not Korean:\n",
    "                tokens = nltk.word_tokenize(sentence)\n",
    "                            \n",
    "            else:\n",
    "                tokens = korean_morphing(sentence)\n",
    "                \n",
    "            for word in tokens:\n",
    "                if has_number(word):\n",
    "                    tokens.remove(word)\n",
    "                        \n",
    "            words.extend(tokens)\n",
    "\n",
    "        words = collections.Counter(words)\n",
    "\n",
    "        dictionary = {}\n",
    "        dictionary['<PAD>'] = 0\n",
    "        dictionary['<UNK>'] = 1\n",
    "        dictionary['<EOS>'] = 2\n",
    "        dictionary['<S>'] = 3\n",
    "        idx = 4\n",
    "        for word in words.most_common():\n",
    "            if len(word[0]) > 0:\n",
    "                dictionary[word[0]] = idx\n",
    "                idx += 1\n",
    "            \n",
    "            if idx >= 20000: break;\n",
    "\n",
    "        return dictionary\n",
    "    \n",
    "def korean_morphing(sentence):\n",
    "    sentence = sentence.replace(' ', ' V ')\n",
    "    word = api.analyze(sentence)\n",
    "\n",
    "    morphs = []\n",
    "    for tokens in word:\n",
    "        for morph in tokens.morphs:\n",
    "            #morphs.append(str(morph))\n",
    "            morphs.append(str(morph.lex))\n",
    "            \n",
    "    return morphs\n",
    "    \n",
    "def clean_text(text):\n",
    "    return re.sub('[\\{\\}\\[\\]\\/,;:|\\)*~`^\\-_+<>@\\#$%&\\\\\\=\\(\\\"“”◀▶【©】☎]', '', text.lower())\n",
    "\n",
    "def has_number(word):\n",
    "    return any(char.isdigit() for char in word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "※ Data Processing...\n",
      ">> Read Files...\n",
      ">> Make Dictionary...\n",
      ">> Make Dictionary...\n",
      ">> Resizing Dataset...\n",
      "English Dictionary Size:  20000\n",
      "Korean Dictionary Size:  19979\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "eng2kor = Eng2KorData(data_directory)\n",
    "eng, kor = eng2kor.data_processing()\n",
    "\n",
    "eng_dict = eng2kor.make_dictionary(eng)\n",
    "kor_dict = eng2kor.make_dictionary(kor, True)\n",
    "\n",
    "input_dict = {y:x for x,y in eng_dict.items()}\n",
    "output_dict = {y:x for x,y in kor_dict.items()}\n",
    "\n",
    "eng_dict_size = len(eng_dict)\n",
    "kor_dict_size = len(kor_dict)\n",
    "        \n",
    "eng_df = pd.DataFrame(eng, columns=['English'])\n",
    "kor_df = pd.DataFrame(kor, columns=['Korean'])\n",
    "\n",
    "eng2kor = pd.concat([eng_df, kor_df], axis=1)\n",
    "\n",
    "print(\">> Resizing Dataset...\")\n",
    "        \n",
    "for i, eng in enumerate(eng2kor['English']):\n",
    "    if len(eng) > len_limit:\n",
    "        eng2kor.drop(eng2kor.index[i])\n",
    "\n",
    "for i, kor in enumerate(eng2kor['Korean']):\n",
    "    if len(kor) > len_limit:\n",
    "        eng2kor.drop(eng2kor.index[i])\n",
    "\n",
    "print(\"English Dictionary Size: \", eng_dict_size)\n",
    "print(\"Korean Dictionary Size: \", kor_dict_size)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def vectorize_data(data, dictionary, Korean=False):\n",
    "    max_len = 0\n",
    "    vec_sentence = []\n",
    "\n",
    "    for sentence in data:\n",
    "        temp = []\n",
    "        \n",
    "        temp.append(dictionary['<S>'])\n",
    "        \n",
    "        if not Korean:\n",
    "            for word in nltk.word_tokenize(sentence):\n",
    "                if len(word) > 0:\n",
    "                    if word in dictionary:\n",
    "                        temp.append(dictionary[word])\n",
    "                    else:\n",
    "                        temp.append(dictionary['<UNK>'])\n",
    "        else:\n",
    "            \n",
    "            for word in korean_morphing(sentence):\n",
    "                if len(word) > 0:\n",
    "                    if word in dictionary:\n",
    "                        temp.append(dictionary[word])\n",
    "                    else:\n",
    "                        temp.append(dictionary['<UNK>'])\n",
    "            \n",
    "        temp.append(dictionary['<EOS>'])\n",
    "        \n",
    "        if max_len < len(temp):\n",
    "            max_len = len(temp)\n",
    "\n",
    "        vec_sentence.append(temp)\n",
    "        \n",
    "    return vec_sentence, max_len\n",
    "\n",
    "def add_padding(vec_data, max_len):\n",
    "    for sentence in vec_data:\n",
    "        for i in range(len(sentence), max_len + 1):\n",
    "            sentence.append(0)\n",
    "    \n",
    "    return np.array(vec_data)\n",
    "\n",
    "eng_vec, eng_max = vectorize_data(eng2kor['English'], eng_dict)\n",
    "kor_vec, kor_max = vectorize_data(eng2kor['Korean'], kor_dict, True)\n",
    "\n",
    "x_train = add_padding(eng_vec, eng_max)\n",
    "y_train = add_padding(kor_vec, kor_max)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(max_len):\n",
    "    PE = np.array([\n",
    "        [pos / np.power(10000, 2 * (i // 2) / d_model) for i in range(d_model)]\n",
    "        if pos != 0 else np.zeros(d_model) for pos in range(max_len)\n",
    "    ]) # np.power(10000, 2 * (j // 2) / d_emb) ??? Why?\n",
    "    PE[1:, 0::2] = np.sin(PE[1:, 0::2]) # 2i\n",
    "    PE[1:, 1::2] = np.cos(PE[1:, 1::2]) # 2i + 1\n",
    "    \n",
    "    return PE\n",
    "\n",
    "class MultiHeadAttention:\n",
    "    def __init__(self):\n",
    "        self.Q_linear_transform_layers = []\n",
    "        self.K_linear_transform_layers = []\n",
    "        self.V_linear_transform_layers = []\n",
    "        \n",
    "        for _ in range(head):\n",
    "            self.Q_linear_transform_layers.append(Dense(d_k, use_bias=False))\n",
    "            self.K_linear_transform_layers.append(Dense(d_k, use_bias=False))\n",
    "            self.V_linear_transform_layers.append(Dense(d_v, use_bias=False))\n",
    "        \n",
    "        self.normarlization_layer = LayerNormalization()\n",
    "        self.output_linear_transfrom_layer = Dense(d_model)\n",
    "        \n",
    "    def __call__(self, Q, K, V, mask=None):\n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(head):\n",
    "            WQ = self.Q_linear_transform_layers[i](Q)\n",
    "            WK = self.K_linear_transform_layers[i](K)\n",
    "            WV = self.V_linear_transform_layers[i](V)\n",
    "            output = self.scaled_dot_product_attention(WQ, WK, WV, mask)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output_result = Concatenate()(outputs)\n",
    "        output = self.output_linear_transfrom_layer(output_result)\n",
    "        output = Dropout(dropout)(output)\n",
    "        output = Add()([output, Q])\n",
    "        \n",
    "        return self.normarlization_layer(output)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        K_T = Lambda(lambda x:tf.transpose(x, perm=[0, 2, 1]))(K)\n",
    "        attention = Lambda(lambda x:tf.matmul(x[0], x[1]) / np.sqrt(d_model))([Q, K_T])\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask_value = Lambda(lambda x: (-1e+10) * (1 - x))(mask)\n",
    "            attention = Add()([attention, mask_value])\n",
    "            \n",
    "        attention = Activation('softmax')(attention)\n",
    "        attention = Dropout(dropout)(attention)\n",
    "        output = Lambda(lambda x:tf.matmul(x[0], x[1]))([attention, V])\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class PositionWiseFeedForwardNetwork:\n",
    "    def __init__(self):\n",
    "        self.w_1 = Conv1D(512, 1, activation='relu')\n",
    "        self.w_2 = Conv1D(512, 1)\n",
    "#         self.linear_transform_layer_1 = Dense(d_ff, input_shape=(d_model,))\n",
    "#         self.relu_layer = Dense(d_ff, activation='relu')\n",
    "#         self.linear_transform_layer_2 = Dense(d_model, input_shape=(d_ff,))\n",
    "        self.normarlization_layer = LayerNormalization()\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        output = self.w_1(x)\n",
    "        output = self.w_2(output)\n",
    "#         output = self.linear_transform_layer_1(x)\n",
    "#         output = self.relu_layer(output)\n",
    "#         output = self.linear_transform_layer_2(output)\n",
    "        output = Dropout(dropout)(output)\n",
    "        output = Add()([output, x])\n",
    "        \n",
    "        return self.normarlization_layer(output)\n",
    "    \n",
    "    \n",
    "    \n",
    "def GetPadMask(q, k):\n",
    "    ones = K.expand_dims(K.ones_like(q, 'float32'), -1)\n",
    "    mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')\n",
    "    mask = K.batch_dot(ones, mask, axes=[2, 1])\n",
    "    return mask\n",
    "\n",
    "\n",
    "def GetSubMask(s):\n",
    "    len_s = tf.shape(s)[1]\n",
    "    bs = tf.shape(s)[:1]\n",
    "    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "    \n",
    "class EncoderLayer:\n",
    "    def __init__(self):\n",
    "        self.multi_head_attention_layer = MultiHeadAttention()\n",
    "        self.position_wise_feed_forward_network = PositionWiseFeedForwardNetwork()\n",
    "        \n",
    "    def __call__(self, encoder_input, mask=None):\n",
    "        encoder_output = self.multi_head_attention_layer(encoder_input, encoder_input, encoder_input, mask)\n",
    "        encoder_output = self.position_wise_feed_forward_network(encoder_output)\n",
    "        \n",
    "        return encoder_output\n",
    "    \n",
    "class Encoder:\n",
    "    def __init__(self):\n",
    "        self.input_embedding =  Embedding(eng_dict_size, d_model)\n",
    "        self.positional_embedding = Embedding(len_limit, d_model, trainable=False, weights=[positional_encoding(len_limit)])\n",
    "        self.layers = [EncoderLayer() for _ in range(N)]\n",
    "        \n",
    "    def __call__(self, encoder_input, source_position):\n",
    "        encoder_output = Add()([self.input_embedding(encoder_input), self.positional_embedding(source_position)])\n",
    "\n",
    "        for layer in self.layers:\n",
    "            encoder_output = layer(encoder_output)\n",
    "            \n",
    "        return encoder_output\n",
    "    \n",
    "class DecoderLayer:\n",
    "    def __init__(self):\n",
    "        self.masked_multi_head_attention_layer = MultiHeadAttention()\n",
    "        self.multi_head_attention_layer = MultiHeadAttention()\n",
    "        self.position_wise_feed_forward_network = PositionWiseFeedForwardNetwork()\n",
    "        \n",
    "    def __call__(self, decoder_input, encoder_output, self_mask=None, enc_mask=None):\n",
    "        decoder_output = self.masked_multi_head_attention_layer(decoder_input, decoder_input, decoder_input, self_mask)\n",
    "        decoder_output = self.multi_head_attention_layer(decoder_output, encoder_output, encoder_output, enc_mask)\n",
    "        decoder_output = self.position_wise_feed_forward_network(decoder_output)\n",
    "        \n",
    "        return decoder_output\n",
    "        \n",
    "class Decoder:\n",
    "    def __init__(self):\n",
    "        self.output_embedding =  Embedding(kor_dict_size, d_model)\n",
    "        self.positional_embedding = Embedding(len_limit, d_model, trainable=False, weights=[positional_encoding(len_limit)])\n",
    "        self.layers = [DecoderLayer() for _ in range(N + 3)]\n",
    "        \n",
    "    def __call__(self, decoder_input, target_position, encoder_input, encoder_output):\n",
    "        decoder_output = Add()([self.output_embedding(decoder_input), self.positional_embedding(target_position)])\n",
    "        \n",
    "        if target_position is not None:\n",
    "            position = self.positional_embedding(target_position)\n",
    "            decoder_output = Add()([decoder_output, position])\n",
    "            \n",
    "        self_pad_mask = Lambda(lambda x: GetPadMask(x, x))(decoder_input)\n",
    "        self_sub_mask = Lambda(GetSubMask)(decoder_input)\n",
    "        self_mask = Lambda(lambda x: K.minimum(x[0], x[1]))([self_pad_mask, self_sub_mask])\n",
    "        enc_mask = Lambda(lambda x: GetPadMask(x[0], x[1]))([decoder_input, encoder_input])\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            decoder_output = layer(decoder_output, encoder_output, self_mask, enc_mask)\n",
    "        \n",
    "        return decoder_output\n",
    "    \n",
    "class Transformer:\n",
    "    def __init__(self):\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.linear_transform_layer = Dense(kor_dict_size, use_bias=False)\n",
    "        #self.softmax_layer = Dense(kor_dict_size, activation='softmax')\n",
    "        #self.softmax_layer = Activation('softmax')\n",
    "    \n",
    "    def compile(self, optimizer='adam'):\n",
    "        print(\">> Start Compile ===========================================================================\")\n",
    "        source_input = Input(shape=(None,), dtype='int32')\n",
    "        target_input = Input(shape=(None,), dtype='int32')\n",
    "        \n",
    "        source_sequence = source_input\n",
    "        target_sequence = Lambda(lambda x:x[:, :-1])(target_input)\n",
    "        target_true = Lambda(lambda x:x[:, 1:])(target_input)\n",
    "        \n",
    "        def get_sequence_position(x):\n",
    "            mask = K.cast(K.not_equal(x, 0), 'int32')\n",
    "            pos = K.cumsum(K.ones_like(x, 'int32'), 1)\n",
    "            return pos * mask\n",
    "        \n",
    "        print(\">> Set Encoder =============================================================================\")\n",
    "        source_position = Lambda(get_sequence_position)(source_sequence)\n",
    "        encoder_output = self.encoder(source_sequence, source_position)\n",
    "        print(\">> Set Decoder =============================================================================\")\n",
    "        target_position = Lambda(get_sequence_position)(target_sequence)\n",
    "        decoder_output = self.decoder(target_sequence, target_position, source_sequence, encoder_output)\n",
    "        final_output = self.linear_transform_layer(decoder_output)\n",
    "        #final_output = self.softmax_layer(final_output)\n",
    "        \n",
    "        def get_loss(args):\n",
    "            y_pred, y_true = args\n",
    "            y_true = tf.cast(y_true, 'int32')\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "            mask = tf.cast(tf.not_equal(y_true, 0), 'float32')\n",
    "            loss = tf.reduce_sum(loss * mask, -1) / tf.reduce_sum(mask, -1)\n",
    "            loss = K.mean(loss)\n",
    "            return loss\n",
    "\n",
    "        def get_accu(args):\n",
    "            y_pred, y_true = args\n",
    "            mask = tf.cast(tf.not_equal(y_true, 0), 'float32')\n",
    "            corr = K.cast(K.equal(K.cast(y_true, 'int32'), K.cast(K.argmax(y_pred, axis=-1), 'int32')), 'float32')\n",
    "            corr = K.sum(corr * mask, -1) / K.sum(mask, -1)\n",
    "            return K.mean(corr)\n",
    "        \n",
    "        print(\">> Set Loss ================================================================================\")\n",
    "\n",
    "        loss = Lambda(get_loss)([final_output, target_true])\n",
    "        self.ppl = Lambda(K.exp)(loss)\n",
    "        self.accu = Lambda(get_accu)([final_output, target_true])\n",
    "        \n",
    "        print(\">> Set Model ===============================================================================\")\n",
    "\n",
    "        self.model = Model([source_input, target_input], loss)\n",
    "        self.model.add_loss([loss])\n",
    "        self.output_model = Model([source_input, target_input], final_output)\n",
    "        \n",
    "        self.model.compile(optimizer, None)\n",
    "        self.model.metrics_names.append('ppl')\n",
    "        self.model.metrics_tensors.append(self.ppl)\n",
    "        self.model.metrics_names.append('accu')\n",
    "        self.model.metrics_tensors.append(self.accu)\n",
    "        \n",
    "    def decode_sequence(self, input_seq, delimiter=''):\n",
    "        \n",
    "        def get_loss(args):\n",
    "            y_pred, y_true = args\n",
    "            y_true = tf.cast(y_true, 'int32')\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "            mask = tf.cast(tf.not_equal(y_true, 0), 'float32')\n",
    "            loss = tf.reduce_sum(loss * mask, -1) / tf.reduce_sum(mask, -1)\n",
    "            loss = K.mean(loss)\n",
    "            return loss\n",
    "        \n",
    "        src_seq, _ = vectorize_data(input_seq, eng_dict)\n",
    "        decoded_tokens = []\n",
    "        target_seq = np.zeros((1, len_limit), dtype='int32')\n",
    "        target_seq[0, 0] = kor_dict['<S>']\n",
    "        \n",
    "        for i in range(len_limit - 1):\n",
    "            output = self.output_model.predict([src_seq, target_seq])\n",
    "            sampled_index = np.argmax(output[0, i, :])\n",
    "            sampled_token = output_dict[sampled_index]\n",
    "            print(output[0, i, :10])\n",
    "            print(sampled_token)\n",
    "            decoded_tokens.append(sampled_token)\n",
    "            \n",
    "            if sampled_index == kor_dict['<EOS>']: break\n",
    "                \n",
    "            target_seq[0, i + 1] = sampled_index\n",
    "        \n",
    "        print('Final::', decoded_tokens)        \n",
    "        \n",
    "        return delimiter.join(decoded_tokens[:-1])\n",
    "\n",
    "class LearningRateScheduler(Callback):\n",
    "    def __init__(self, d_model, warmup=4000):\n",
    "        self.basic = d_model**-0.5\n",
    "        self.warm = warmup**-1.5\n",
    "        self.step_num = 0\n",
    "\n",
    "    def on_batch_begin(self, batch, logs = None):\n",
    "        self.step_num += 1\n",
    "        lr = self.basic * min(self.step_num**-0.5, self.step_num*self.warm)\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/moon/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/moon/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Start Compile ===========================================================================\n",
      ">> Set Encoder =============================================================================\n",
      "WARNING:tensorflow:From /home/moon/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/moon/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Set Decoder =============================================================================\n",
      ">> Set Loss ================================================================================\n",
      ">> Set Model ===============================================================================\n",
      "x_train Shape:  (54501, 30)\n",
      "y_train Shape:  (54501, 91)\n",
      "x_test Shape:  (6056, 30)\n",
      "y_test Shape:  (6056, 91)\n",
      "English Dictionary Size:  20000\n",
      "Korean Dictionary Size:  19979\n",
      ">> Start Training\n",
      "WARNING:tensorflow:From /home/moon/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/moon/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/moon/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/moon/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54501 samples, validate on 6056 samples\n",
      "Epoch 1/30\n",
      "32960/54501 [=================>............] - ETA: 3:08 - loss: 6.2738 - ppl: 2302.0575 - accu: 0.2410"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    transformer = Transformer()\n",
    "\n",
    "    transformer.compile(Adam(0.001, 0.9, 0.98, epsilon=1e-9))\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.1, shuffle=True)     \n",
    "\n",
    "    print(\"x_train Shape: \", x_train.shape)\n",
    "    print(\"y_train Shape: \", y_train.shape)\n",
    "    print(\"x_test Shape: \", x_test.shape)\n",
    "    print(\"y_test Shape: \", y_test.shape)\n",
    "    print(\"English Dictionary Size: \", eng_dict_size)\n",
    "    print(\"Korean Dictionary Size: \", kor_dict_size)\n",
    "\n",
    "    print(\">> Start Training\")\n",
    "\n",
    "    learning_rate_scheduler = LearningRateScheduler(d_model, warmup_steps)\n",
    "\n",
    "    transformer.model.fit([x_train, y_train], None, batch_size=64, epochs=epochs,\n",
    "                         validation_data=([x_test, y_test], None), callbacks=[learning_rate_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2kor.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 18\n",
    "\n",
    "eng = []\n",
    "for i in x_train[num]:\n",
    "    eng.append(input_dict[i])\n",
    "    \n",
    "print(eng)\n",
    "\n",
    "kor = []\n",
    "for i in y_train[num]:\n",
    "    kor.append(output_dict[i])\n",
    "    \n",
    "print(kor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text = [\"the field was filled with snow .\"]\n",
    "\n",
    "print(text)\n",
    "print(vectorize_data(text, eng_dict))\n",
    "transformer.decode_sequence(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
